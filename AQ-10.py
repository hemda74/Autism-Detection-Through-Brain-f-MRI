# -*- coding: utf-8 -*-
"""capstone-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y1NzzZYmA68nwsz_qtpyslHeDdx3sNiK

# A_10 Project: 
## *Machine learning approaches to the classification problem for autism spectrum disorder*

We'll be using the "Autistic Spectrum Disorder Screening Data for Adult And Toddels" public dataset from the 
[`UCI repository`](http://archive.ics.uci.edu/ml/datasets/Autism+Screening+Adult#). The datset was donated by Prof. Fadi Thabtah, after being published in the article "Autism Spectrum Disorder Screening: Machine Learning
Adaptation and DSM-5 Fulfillment". You can find the article by Fadi Thabtah [`online`](https://dl.acm.org/citation.cfm?id=3107515). The data we investigate here consists of small changes to the original dataset, such as removing the 'age_desc' feature and records with missing or ill-formatted entries. Very few published articles are available online as the dataset has not been out for long and as a result refernce to a benchmark model is very rare.


**Abstract:** 
Autistic Spectrum Disorder (ASD) is a neurodevelopment  condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis.  

The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.

## Road Map:

* [Step 0](#step0): Import Datasets.

* [Step 1](#step1): Clean Datasets (The data needs to be cleaned; many rows contain missing data, and there may be erroneous data identifiable as outliers).

* [Step 2](#step2): A quick visualization with *Seaborn*.

* [Step 3](#step3): At First, We applied several Supervised Machine Learning (SML) techniques on the data for classification purpose.

* [Step 4](#step4): Next, We experimented with different topologies, optimizers, and hyperparameters for different models.

* [Step 5](#step5): Model tuning.

* [Step 6](#step6): Feature Selection.

* [Step 7](#step7): Then We built a Multi-Layer Perceptron and train it to classify indivisual with ASD based on its features.

* [Step 8](#step8): Conclusion.

## Let's begin with preparing our data set.

---
<a id='step0'></a>
## Step 0: Import Datasets

 We start by importing the 'datasetcsv.csv' file into a Pandas dataframe and take a look at it.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import libraries necessary for this project
import numpy as np
import pandas as pd
from time import time
#from IPython.display import display # Allows the use of display() for DataFrames
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.core.common import random_state
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
from sklearn import neighbors
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
# Import supplementary visualization code visuals.py
#import visuals as vs

# Pretty display for notebooks
# %matplotlib inline
#importing file 
#from google.colab import files
#uploaded =files.upload()
data=pd.read_csv('datasetcsv.csv')
data.head(7)

# Total number of records
n_records = len(data.index)
print(n_records)
# count number of rows and cloumn
rowsnum=data.shape[0]
colsnum=data.shape[1]
#count the classification column
print(data['Class/ASD'].value_counts())

#  Number of records where individual's with ASD
#n_asd_yes = len(df[data['Class/ASD Traits'] == 'YES'])

# Number of records where individual's with no ASD
#n_asd_no = len(df[df['Class/ASD Traits'] == 'NO'])

# Percentage of individuals whose are with ASD
#yes_percent = float(n_asd_yes) / n_records *100

# Print the results
#print ("Total number of records: {}"+(n_records))
#print ("Individuals diagonised with ASD: {}"+(n_asd_yes))
#print ("Individuals not diagonised with ASD: {}"+(n_asd_no))
#print ("Percentage of individuals diagonised with ASD: {:.2f}%"+(yes_percent))

"""** Featureset Exploration **

This data contains 704 instances, and contains the following attributes:
* **age**: *number* (Age in years). 
* **gendar**: *String* [Male/Female]. 
* **ethnicity**: *String* (List of common ethnicities in text format). 
* **Born with jaundice**: *Boolean* [yes or no]. 
* **Family member with PDD**: *Boolean* [yes or no]. 
* **Who is completing the test**: *String* [Parent, self, caregiver, medical staff, clinician ,etc.].
* **Country of residence **: *String* (List of countries in text format).
* **Used the screening app before **: *Boolean* [yes or no] (Whether the user has used a screening app) 
* **Screening Method Type**: *Integer* [0,1,2,3] (The type of screening methods chosen based on age category (0=toddler, 1=child, 2= adolescent, 3= adult).
* **Question 1-10 Answer**: *Binary* [0, 1] (The answer code of the question based on the screening method used).
* **Screening Score**: *Integer* (The final score obtained based on the scoring algorithm of the screening method used. This was computed in an automated manner).

----
## Preparing the Data
Before data can be used as input for machine learning algorithms, it must be cleaned, formatted, and maybe even restructured â€” this is typically known as **preprocessing**. Unfortunately, for this dataset, there are many invalid or missing entries(?) we must deal with, moreover, there are some qualities about certain features that must be adjusted. This preprocessing can help tremendously with the outcome and predictive power of nearly all learning algorithms.

I use the optional parmaters in read_csv to convert missing data (indicated by a ?) into NaN, and to add the appropriate column names ():
"""

asd_data = pd.read_csv('datasetcsv.csv', na_values=['?'])
asd_data.head(n=5)

"""Here I evaluate whether the data needs cleaning; your model is only as good as the data it's given."""

asd_data.describe()

"""There are quite a few missing values in the data set. Before I just drop every row that's missing data, I make sure not to bias the data in any way. In other words we need to make sure that there does not appear to be any sort of correlation to what sort of data has missing fields. If there were, I'd have to try and go back and fill that data in.

---
<a id='step1'></a>
## Step 1: Clean Datasets
"""

asd_data.loc[(asd_data['age'].isnull()) |(asd_data['gender'].isnull()) |(asd_data['ethnicity'].isnull()) 
|(asd_data['jundice'].isnull())|(asd_data['autsim'].isnull()) |(asd_data['contry_of_res'].isnull())
            |(asd_data['used_app_before'].isnull())|(asd_data.iloc[:,18].isnull())|
             (asd_data['age_desc'].isnull())
            |(asd_data['relation'].isnull())]

"""Since the missing data seems randomly distributed, I go ahead and drop rows with missing data. """

asd_data.dropna(inplace=True)
asd_data.describe()

"""### If we could have fill with median values for 'NaN' instead of dropping them, but in this situation that is little complicated as I have lot of categorical colums with 'NaN'."""

asd_data.median()
numerical = ['age', 'result']
asd_data.fillna(asd_data[numerical].median())
asd_data.describe()

"""Let's check out the data types of all our features including the target feature. Moreover, lets count the total number of instances and the target-class distribution."""

# Reminder of the features:
print(asd_data.dtypes)


# Total number of records in clean dataset
n_records = len(asd_data.index)

# TODO: Number of records where individual's with ASD in the clean dataset
n_asd_yes = len(asd_data[asd_data['Class/ASD'] == 'YES'])

# TODO: Number of records where individual's with no ASD in the clean dataset
n_asd_no = len(asd_data[asd_data['Class/ASD'] == 'NO'])

# Print the results
print ("Total number of records: {}".format(n_records))
print ("Individuals diagonised with ASD: {}".format(n_asd_yes))
print ("Individuals not diagonised with ASD: {}".format(n_asd_no))

"""---
<a id='step2'></a>

## Step 2: A quick visualization with *Seaborn*
"""

import seaborn as sns
import matplotlib.pyplot as plt
sns.set(style="whitegrid", color_codes=True)
sns.countplot(data['Class/ASD'],label='count')

# Draw a nested violinplot and split the violins for easier comparison
sns.violinplot(x="jundice", y="result", hue="autsim", data=asd_data, split=True,
                inner="quart", palette={'yes': "r", 'no': "b"})
sns.despine(left=True)

#Draw a nested violinplot and split the violins for easier comparison
sns.violinplot(x="gender", y="result", hue="Class/ASD", data=asd_data, split=True,
                inner="quart", palette={'YES': "r", 'NO': "b"})
sns.despine(left=True)

sns.factorplot(x="jundice", y="result", hue="Class/ASD", col="gender", data=asd_data, kind="swarm");

sns.factorplot(x="gender", y="result", hue="Class/ASD",
               col="relation", data=asd_data, kind="box", size=4, aspect=.5, palette={'YES': "r", 'NO': "b"});

g = sns.factorplot(x="result", y="jundice",
                   hue="gender", row="relation",
                   data=asd_data,
                    orient="h", size=2, aspect=3.5, palette={'f': "r", 'm': "b"},
                  kind="violin", dodge=True, cut=0, bw=.2)

"""In the two figures above, I have used two different depiction techniques to have a quick peek on the ASD dataset we are dealing with. Both of the occasions I have used `factorplot` module from `seaborn` visualization software. In the first case, I used '`swamp`' kind of graph expressing the relationship between several different features present in the data whereas in the second case, '`box`' method was used to present the visual way of showing how different features were associated with each other.

Next I'll need to convert the Pandas dataframes into numpy arrays that can be used by scikit_learn. Let's create an array that extracts only the feature data we want to work with and another array that contains the classes (class/ASD).
"""

# Split the data into features and target label
asd_raw = asd_data['Class/ASD']
features_raw = asd_data[['age', 'gender', 'ethnicity', 'jundice', 'autsim', 'contry_of_res', 'result',
                      'relation','A1_Score','A2_Score','A3_Score','A4_Score','A5_Score','A6_Score','A7_Score','A8_Score',
                      'A9_Score','A10_Score']]

"""Some of our models require the input data to be normalized, so I proceed to normalize the attribute data. Here, I use preprocessing.MinMaxScaler()."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
numerical = ['age', 'result']

features_minmax_transform = pd.DataFrame(data = features_raw)
features_minmax_transform[numerical] = scaler.fit_transform(features_raw[numerical])
features_minmax_transform
# Show an example of a record with scaling applied
display(features_minmax_transform.head(n = 5))

"""### One-Hot-Coding

From the table in **Clean Data Sets** above, we can see there are several features for each record that are non-numeric such as `Country_of_residence`, `ethnicity` etc. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. One popular way to convert categorical variables is by using the **one-hot encoding** scheme. One-hot encoding creates a _"dummy"_ variable for each possible category of each non-numeric feature. For example, assume `someFeature` has three possible entries: `A`, `B`, or `C`. We then encode this feature into `someFeature_A`, `someFeature_B` and `someFeature_C`.

|   | someFeature |                    | someFeature_A | someFeature_B | someFeature_C |
| :-: | :-: |                            | :-: | :-: | :-: |
| 0 |  B  |  | 0 | 1 | 0 |
| 1 |  C  | ----> one-hot encode ----> | 0 | 0 | 1 |
| 2 |  A  |  | 1 | 0 | 0 |

Additionally, as with the non-numeric features, I need to convert the non-numeric target label, `'Class/ASD'` to numerical values for the learning algorithm to work. Since there are only two possible categories for this label ("YES" and "NO" to Class/ASD), I can avoid using one-hot encoding and simply encode these two categories as `0` and `1`, respectively. In code cell below, I will implement the following:
 - Use [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) to perform one-hot encoding on the `'features_minmax_transform'` data.
 - Convert the target label `'asd_raw'` to numerical entries.
   - Set records with "NO" to `0` and records with "YES" to `1`.
"""

#One-hot encode the 'features_minmax_transform' data using pandas.get_dummies()
features_final = pd.get_dummies(features_minmax_transform)
display(features_final.head(5))


# Encode the 'all_classes_raw' data to numerical values
asd_classes = asd_raw.apply(lambda x: 1 if x == 'YES' else 0)



# Print the number of features after one-hot encoding
encoded = list(features_final.columns)
print ("{} total features after one-hot encoding.".format(len(encoded)))

# Uncomment the following line to see the encoded feature names
print(encoded)

# histogram of Class/ASD

# 8 bins
plt.hist(asd_classes, bins=10)

# x-axis limit from 0 to 1
plt.xlim(0,1)
plt.title('Histogram of Class/ASD')
plt.xlabel('Class/ASD from processed data')
plt.ylabel('Frequency')

"""### Shuffle and Split Data
Now all _categorical variables_ have been converted into numerical features, and all numerical features have been normalized. As always, I will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.
"""

from sklearn.model_selection import train_test_split

np.random.seed(1234)

X_train, X_test, y_train, y_test = train_test_split(features_final, asd_classes, train_size=0.80, random_state=1)


# Show the results of the split
print ("Training set has {} samples.".format(X_train.shape[0]))
print ("Testing set has {} samples.".format(X_test.shape[0]))
asd_data

"""---
<a id='step3'></a>

## Step 3: Models

###  Supervised Learning Models
**I have applied the following supervised learning models in this project which are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html).

[(1)](#1): Decision Trees

[(2)](#2) Random Forest

[(3)](#3) Support Vector Machines (SVM)

[(4)](#4) K-Nearest Neighbors (KNeighbors)

[(5)](#5) Gaussian Naive Bayes (GaussianNB)

[(6)](#6) Logistic Regression

[(7)](#7) Linear Discriminant Analysis (LDA)

[(8)](#8) Quadratic Discriminant Analysis (QDA)

In the two figures above, I have used two different depiction techniques to have a quick peek on the ASD dataset we are dealing with. Both of the occasions I have used `factorplot` module from `seaborn` visualization software. In the first case, I used '`swamp`' kind of graph expressing the relationship between several different features present in the data whereas in the second case, '`box`' method was used to present the visual way of showing how different features were associated with each other.

---
<a id='1'></a>

##  (1) Decision Trees

I start with creating a DecisionTreeClassifier and fit it to the training data.
"""

from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

dectree = DecisionTreeClassifier(
    
)

# Train the classifier on the training set
dectree.fit(X_train, y_train)

"""### Depiction of Decision Tree algorithm"""

import pydotplus 


dot_data = tree.export_graphviz(dectree,
                                out_file=None,
                                filled=True,
                                rounded=True,
                                special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  

from IPython.display import Image 
Image(graph.create_png())

"""---------------
## Evaluating Model Performance

### Metrics 
We can use **F-beta score** as a metric that considers both precision and recall:

$$ F_{\beta} = (1 + \beta^2) \cdot \frac{precision \cdot recall}{\left( \beta^2 \cdot precision \right) + recall} $$

In particular, when $\beta = 0.5$, more emphasis is placed on precision. This is called the **F$_{0.5}$ score** (or F-score for simplicity).

#### Note: Recap of accuracy, precision, recall

** Accuracy ** measures how often the classifier makes the correct prediction. Itâ€™s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).

** Precision ** tells us what proportion of messages we classified as spam, actually were spam.
It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classificatio), in other words it is the ratio of

`[True Positives/(True Positives + False Positives)]`

** Recall (sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.
It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of

`[True Positives/(True Positives + False Negatives)]`

For classification problems that are skewed in their classification distributions like in our case where we have 
- a total of 609 records with 
- 180 individuals diagonised with ASD and 
- 429 individuals not diagonised with ASD

accuracy by itself is not a very good metric. 
Thus, in this case precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios).
"""

# make class predictions for the testing set
y_pred_class = dectree.predict(X_test)

# print the first 25 true and predicted responses
print('True:', y_test.values[0:25])
print('False:', y_pred_class[0:25])

from sklearn import metrics
# IMPORTANT: first argument is true values, second argument is predicted values
# this produces a 2x2 numpy array (matrix)
#print(metrics.confusion_matrix(y_test, y_pred_class))

# save confusion matrix and slice into four pieces
confusion = metrics.confusion_matrix(y_test, y_pred_class)
print(confusion)
#[row, column]
TP = confusion[1, 1]
TN = confusion[0, 0]
FP = confusion[0, 1]
FN = confusion[1, 0]

"""### Metrics computed from a confusion matrix

**Classification Accuracy**: Overall, how often is the classifier correct?
"""

# use float to perform true division, not integer division
print((TP + TN) / float(TP + TN + FP + FN))

"""**Classification Error**: Overall, how often is the classifier incorrect?"""

classification_error = (FP + FN) / float(TP + TN + FP + FN)

print(classification_error)

"""**Sensitivity**: When the actual value is positive, how often is the prediction correct?

"""

sensitivity = TP / float(FN + TP)

print(sensitivity)
print(metrics.recall_score(y_test, y_pred_class))

"""**Specificity**: When the actual value is negative, how often is the prediction correct?"""

specificity = TN / (TN + FP)

print(specificity)

"""**False Positive Rate**: When the actual value is negative, how often is the prediction incorrect?"""

false_positive_rate = FP / float(TN + FP)

print(false_positive_rate)
#print(1 - specificity)

"""**Precision**: When a positive value is predicted, how often is the prediction correct?"""

precision = TP / float(TP + FP)

#print(precision)
print(metrics.precision_score(y_test, y_pred_class))

"""### Visualizing the classification prediction:"""

# print the first 10 predicted responses
# 1D array (vector) of binary values (0, 1)
dectree.predict(X_test)[0:10]

# print the first 10 predicted probabilities of class membership
dectree.predict_proba(X_test)[0:10]

# store the predicted probabilities for class 1
y_pred_prob = dectree.predict_proba(X_test)[:, 1]

# allow plots to appear in the notebook

import matplotlib.pyplot as plt

# adjust the font size 
plt.rcParams['font.size'] = 12

# histogram of predicted probabilities

# 8 bins
plt.hist(y_pred_prob, bins=10)

# x-axis limit from 0 to 1
plt.xlim(0,1)
plt.title('Histogram of predicted probabilities')
plt.xlabel('Predicted probability of ASD')
plt.ylabel('Frequency')

"""### Receiver Operating Characteristic (ROC) Curves

It would be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold? Lets plot the ROC curve.
"""

# IMPORTANT: first argument is true values, second argument is predicted probabilities

# we pass y_test and y_pred_prob
# we do not use y_pred_class, because it will give incorrect results without generating an error
# roc_curve returns 3 objects fpr, tpr, thresholds
# fpr: false positive rate
# tpr: true positive rate
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)

plt.plot(fpr, tpr)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.rcParams['font.size'] = 12
plt.title('ROC curve for diabetes classifier')
plt.xlabel('False Positive Rate (1 - Specificity)')
plt.ylabel('True Positive Rate (Sensitivity)')
plt.grid(True)

"""#### Score metric for Model performance"""

dectree.score(X_test, y_test)

"""### Cross-validation:

Now instead of a single train/test split, I use K-Fold cross validation to get a better measure of your model's accuracy (K=10). 
"""

from sklearn.model_selection import cross_val_score

dectree = DecisionTreeClassifier(random_state=1)

cv_scores = cross_val_score(dectree, features_final, asd_classes, cv=10)

cv_scores.mean()

"""### AUC Score: 

AUC is the percentage of the ROC plot that is underneath the curve
"""

# calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cross_val_score(dectree, features_final, asd_classes, cv=10, scoring='roc_auc').mean()

"""### F-beta Score:"""

dectree.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = dectree.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='2'></a>
## (2) Random Forest

Now I apply a **RandomForestClassifier** instead to see whether it performs better.
"""

from sklearn.ensemble import RandomForestClassifier

ranfor = RandomForestClassifier(n_estimators=5, random_state=1)
cv_scores = cross_val_score(ranfor, features_final, asd_classes, cv=10)
cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

#calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cross_val_score(ranfor, features_final, asd_classes, cv=10, scoring='roc_auc').mean()

"""F-beta Score:"""

ranfor.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = ranfor.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='3'></a>
## (3) SVM

Next I try using svm.SVC with a linear kernel and see how well it does in comparison to the decision tree.
"""

from sklearn import svm

C = 1.0
svc = svm.SVC(kernel='linear', C=C, gamma=2)

cv_scores = cross_val_score(svc, features_final, asd_classes, cv=10)

cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cross_val_score(svc, features_final, asd_classes, cv=10, scoring='roc_auc').mean()

"""F-beta Score:"""

svc.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = svc.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='4'></a>

## (4) K-Nearest-Neighbors (KNN)
Next, I explore the K-Nearest-Neighbors algorithm with a starting value of K=10. Recall that K is an example of a hyperparameter - a parameter on the model itself which may need to be tuned for best results on your particular data set.
"""

from sklearn import neighbors

knn = neighbors.KNeighborsClassifier(n_neighbors=10)
cv_scores = cross_val_score(knn, features_final, asd_classes, cv=10)

cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cross_val_score(knn, features_final, asd_classes, cv=10, scoring='roc_auc').mean()

"""F-beta Score:"""

knn.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = knn.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""Choosing K is tricky, so I can't discard KNN until we've tried different values of K. Hence we write a for loop to run KNN with K values ranging from 10 to 50 and see if K makes a substantial difference. """

for n in range(10, 50):
    knn = neighbors.KNeighborsClassifier(n_neighbors=n)
    cv_scores = cross_val_score(knn, features_final, asd_classes, cv=10)
    print (n, cv_scores.mean())

"""---
<a id='5'></a>

## (5) Naive Bayes

Now I try naive_bayes.MultinomialNB classifier and ask how does its accuracy stack up.
"""

from sklearn.naive_bayes import MultinomialNB

#scaler = preprocessing.MinMaxScaler()
#all_features_minmax = scaler.fit_transform(all_features)

nb = MultinomialNB()
cv_scores = cross_val_score(nb, features_final, asd_classes, cv=10)

cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from sklearn.model_selection import cross_val_score
cross_val_score(nb, features_final, asd_classes, cv=10, scoring='roc_auc').mean()

"""F-beta Score:"""

nb.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = nb.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='6'></a>

## (6) Logistic Regression

We've tried all these fancy techniques, but fundamentally this is just a binary classification problem. Try Logisitic Regression, which is a simple way to tackling this sort of thing.
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()
cv_scores = cross_val_score(logreg, features_final, asd_classes, cv=10)
cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from sklearn.model_selection import cross_val_score
cv_scores_roc = cross_val_score(logreg, features_final, asd_classes, cv=10, scoring='roc_auc').mean()
cv_scores_roc.mean()

"""F-beta Score:"""

logreg.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = logreg.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='7'></a>

## (7) Linear Discriminant Analysis
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
cv_scores = cross_val_score(lda, features_final, asd_classes, cv=10)
cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cv_scores_roc = cross_val_score(lda, features_final, asd_classes, cv=10, scoring='roc_auc').mean()
cv_scores_roc.mean()

"""F-beta Score:"""

lda.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = lda.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='8'></a>
## (8) Quadratic Discriminant Analysis
"""

from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

qda = QuadraticDiscriminantAnalysis()
cv_scores = cross_val_score(qda, features_final, asd_classes, cv=10)
cv_scores.mean()

"""AUC Score: AUC is the percentage of the ROC plot that is underneath the curve"""

# calculate cross-validated AUC
from  sklearn.model_selection import cross_val_score
cv_scores_roc = cross_val_score(qda, features_final, asd_classes, cv=10, scoring='roc_auc').mean()
cv_scores_roc.mean()

"""F-beta Score:"""

qda.fit(X_train, y_train)
from sklearn.metrics import fbeta_score
predictions_test = qda.predict(X_test)
fbeta_score(y_test, predictions_test, average='binary', beta=0.5)

"""---
<a id='step5'></a>

## Step 5: Model Tuning

Now I will fine tune the chosen model. For this I use grid search (GridSearchCV) with at least one important parameter tuned with at least 3 different values. I will need to use the entire training set for this. In the code cell below, I will need to implement the following:

- Import [`sklearn.grid_search.GridSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).
- Initialize the classifier you've chosen and store it in `clf`.
 - Set a `random_state` if one is available to the same state you set before.
- Create a dictionary of parameters you wish to tune for the chosen model.
 - Example: `parameters = {'parameter' : [list of values]}`.
 - **Note:** Avoid tuning the `max_features` parameter of your learner if that parameter is available!
- Use `make_scorer` to create an `fbeta_score` scoring object (with $\beta = 0.5$).
- Perform grid search on the classifier `clf` using the `'scorer'`, and store it in `grid_obj`.
- Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_fit`.



Note that, svm.SVC may perform differently with different kernels. The choice of kernel is an example of a "hyperparamter." Here I experiented with different kernels such as 'rbf', 'sigmoid', and 'poly' and found that the best-performing kernel is `linear`.
"""

# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries
from sklearn.metrics import fbeta_score
from sklearn.metrics import accuracy_score

from sklearn.metrics import make_scorer
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

def f_beta_score(y_true, y_predict):
    return fbeta_score(y_true, y_predict, beta = 0.5)


# TODO: Initialize the classifier
clf = SVC(random_state = 1)

# TODO: Create the parameters list you wish to tune, using a dictionary if needed.
# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}
parameters = {'C':range(1,6),'kernel':['linear','poly','rbf','sigmoid'],'degree':range(1,6)}

# TODO: Make an fbeta_score scoring object using make_scorer()
scorer = make_scorer(f_beta_score)

# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()
grid_obj = GridSearchCV(estimator = clf, param_grid = parameters, scoring = scorer)

# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()
grid_fit = grid_obj.fit(X_train, y_train)

# Get the estimator
best_clf = grid_fit.best_estimator_

# Make predictions using the unoptimized and model
predictions = (clf.fit(X_train, y_train)).predict(X_test)
best_predictions = best_clf.predict(X_test)

# Report the before-and-afterscores
print ("Unoptimized model\n------")
print ("Accuracy score on testing data: {:.4f}".format(accuracy_score(y_test, predictions)))
print ("F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta = 0.5)))
print ("\nOptimized Model\n------")
print ("Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions)))
print ("Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5)))

"""In this `GridSearchCV` methos, we came up with the best result when the kernel was selected as `linear`.

---
<a id='step6'></a>
## Step 6: Feature Importance

An important task when performing supervised learning on a dataset like the autistic data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual has ASD or not.

Choose a scikit-learn classifier (e.g., gradientBoosting, adaboost, random forests) that has a `feature_importance_` attribute, which is a function that ranks the importance of features according to the chosen classifier. In the next python cell fit this classifier to training set and use this attribute to determine the top 5 most important features for the ASD dataset.

###  Extracting Feature Importance
Choose a `scikit-learn` supervised learning algorithm that has a `feature_importance_` attribute availble for it. This attribute is a function that ranks the importance of each feature when making predictions based on the chosen algorithm.

In the code cell below, I will implement the following:
 - Import a supervised learning model from sklearn if it is different from the three used earlier.
 - Train the supervised model on the entire training set.
 - Extract the feature importances using `'.feature_importances_'`.
"""

# TODO: Import a supervised learning model that has 'feature_importances_'
from sklearn.ensemble import GradientBoostingClassifier


# TODO: Train the supervised model on the training set using .fit(X_train, y_train)
model = GradientBoostingClassifier(random_state=0)
model.fit(X_train, y_train)

# TODO: Extract the feature importances using .feature_importances_ 
importances = model.feature_importances_
print(importances)
# Plot
plt.bar(range(len(model.feature_importances_)), model.feature_importances_)
plt.show()

# TODO: Import a supervised learning model that has 'feature_importances_'
from sklearn.ensemble import AdaBoostClassifier


# TODO: Train the supervised model on the training set using .fit(X_train, y_train)
model = AdaBoostClassifier(random_state=0)
model.fit(X_train, y_train)

# TODO: Extract the feature importances using .feature_importances_ 
importances = model.feature_importances_
print(importances)
# Plot



"""### Feature Selection
We need to ask ourselves how does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower â€” at the cost of performance metrics. From the visualization above, we see that the top five most important features(in order with their weightage factor) contribute more than half of the importance of **all** features present in the data. These 5 features are:
- '`result`'
- '`relation_self`'
- '`country_of_residence`'
- '`jundice_no`'
- '`jundice_yes`'

This hints that we can attempt to *reduce the feature space* and simplify the information required for the model to learn. Although looking at those weight factor it seems like '`result`' feature is clearly dominating its influence on the algorithms over all other features.

---
<a id='step7'></a>

## Step 7: Building a MLP  model architecture
In this last part I build a model here using sequential model architecture best known as *Multi Layer Perceptron (MLP)*.
"""

# Imports
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation


np.random.seed(42)

# Building the model architecture with one layer of length 4


model = Sequential()
model.add(Dense(8, activation='relu', input_dim= 94))
model.add(Dropout(0.2))
model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
   
    
model.summary()

# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.
model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# Running and evaluating the model
hist = model.fit(X_train, y_train,
          batch_size=16,
          epochs=100,
          validation_data=(X_test, y_test), 
          verbose=2)

"""### Evaluating the model
This will give you the accuracy of the model. 
"""

# Evaluating the model on the training and testing set
score = model.evaluate(X_train, y_train)
print("\n Training Accuracy:", score[1])

score = model.evaluate(X_test, y_test, verbose=0)
print("\n Testing accuracy: ", score[1])

"""### Free-Flow Visualization:

As with "Feature Selection" exploration, we have seen the attribute named 'result' has a such a powerful presence in the ASD dataset, all other attributes have little to no contribution in deciding the final class. In the Figure  below, I have drawn a scatterplot with 'result' as x-axis and ASD-class (say 'yes' is 1 and 'no' is 0) as y-axis, and reconfirmed that underlying association between the given variables.
"""

asd_data['Class/ASD'] = asd_raw.apply(lambda x: 1 if x == 'YES' else 0)
asd_data['result'] = asd_data['result'].apply(lambda x: (np.log(x + 1)))
asd_data = pd.DataFrame(data = asd_data)
#asd_data['result'] = scaler.fit_transform(asd_data['result'])

#ax = sns.regplot(x= 'result', y='Class/ASD', data = asd_data, fit_reg=False)

sns.stripplot(x="result", y="Class/ASD", data = asd_data, jitter=True);

#sns.swarmplot(x="result", y="Class/ASD", data=asd_data, palette={'YES': "r", 'NO': "b"});

"""---
<a id='step8'></a>

## Step 8: Conclusion


After exploring my `ASD` dataset with different kind of learning algorithms, I have arrived into this conclusion that all of my model work extremely well with the data. I have used three different `metric` (such as `accuracy`, `AUC score` and `F-score`) to measure the performance of my models, and it seems like all of the `metric` indicated an almost perfect classification of the ASD cases. Here I think the reason of this high performances with different models is the fact that only one of the feature is predominant over all others which I confirmed with the [`Feature Importance`](#step6) section in this notebook. 

I think to build a more accurate model, we need to have access to more larger datasets. Here the number of instances after cleaning the data were not so sufficient enough so that I can claim that this model is optimum. As this dataset is only publicly available from Decemeber 2017, I think not many works that deal with this dataset is available online. In that consideration, my models can serve as benchmark models for any machine learning researcher/practitioner who will be interested to explore this dataset further. With this fact in mind, I think this are very well developed model that can detect ASD in indivisuals with certain given attributes.

# Rebuilding the model without the `'result'` variable.
"""

# Split the data into features and target label
asd_raw = asd_data['Class/ASD']
features_raw = asd_data[['age', 'gender', 'ethnicity', 'jundice', 'autsim', 'contry_of_res', 
                      'relation','A1_Score','A2_Score','A3_Score','A4_Score','A5_Score','A6_Score','A7_Score','A8_Score',
                      'A9_Score','A10_Score']]


from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

#One-hot encode the 'features_minmax_transform' data using pandas.get_dummies()
features_final = pd.get_dummies(features_minmax_transform)
display(features_final.head(5))


# Encode the 'all_classes_raw' data to numerical values
asd_classes = asd_raw.apply(lambda x: 1 if x == 'YES' else 0)



# Print the number of features after one-hot encoding
encoded = list(features_final.columns)
print ("{} total features after one-hot encoding.".format(len(encoded)))

# Uncomment the following line to see the encoded feature names
print( encoded)